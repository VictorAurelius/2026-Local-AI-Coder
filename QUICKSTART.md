# üöÄ H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng Nhanh ‚Äî Local AI Coder

> H·ªá th·ªëng AI h·ªó tr·ª£ l·∫≠p tr√¨nh ch·∫°y ho√†n to√†n local v·ªõi Qwen3-Coder 30B

## üìã Th√¥ng Tin H·ªá Th·ªëng

| Th√†nh ph·∫ßn | Th√¥ng tin |
|------------|-----------|
| Model | Qwen3-Coder 30B (A3B) - 30.5B params, 3.3B active |
| Context Window | 256K tokens (~6,000 d√≤ng code) |
| Ollama API | http://localhost:11434 |
| Open WebUI | http://localhost:3000 |
| Docker Containers | `ollama`, `open-webui` |

---

## üéØ C√°ch S·ª≠ D·ª•ng

### 1. Chat qua Terminal

```bash
# Model g·ªëc
docker exec -it ollama ollama run qwen3-coder:30b

# Java Expert persona
docker exec -it ollama ollama run java-expert

# Code Reviewer persona
docker exec -it ollama ollama run code-reviewer
```

**V√≠ d·ª• prompt:**
```
>>> Vi·∫øt Spring Boot 3 REST controller CRUD cho entity Product,
    s·ª≠ d·ª•ng JPA, Jakarta Validation, ResponseEntity,
    v√† custom exception handling
```

Tho√°t: `/bye` ho·∫∑c `Ctrl + D`

---

### 2. Chat qua Web UI

1. M·ªü tr√¨nh duy·ªát: **http://localhost:3000**
2. T·∫°o t√†i kho·∫£n admin (l·∫ßn ƒë·∫ßu, ch·ªâ l∆∞u local)
3. Ch·ªçn model t·ª´ dropdown
4. B·∫Øt ƒë·∫ßu chat ho·∫∑c k√©o th·∫£ file ƒë·ªÉ upload

---

### 3. Review Code

**Review 1 file:**
```bash
./scripts/review.sh src/main/java/com/example/UserService.java
```

**Review c·∫£ folder:**
```bash
./scripts/review.sh src/main/java/com/example/service/
```

**Review v·ªõi prompt t√πy ch·ªânh:**
```bash
./scripts/review.sh OrderController.java "T·∫≠p trung v√†o security v√† SQL injection"
```

**Pipe tr·ª±c ti·∫øp:**
```bash
# 1 file
cat UserService.java | docker exec -i ollama ollama run code-reviewer "Review code n√†y"

# Nhi·ªÅu file
cat src/main/java/com/example/service/*.java | \
  docker exec -i ollama ollama run code-reviewer "Ph√¢n t√≠ch to√†n b·ªô service layer"

# C·∫£ project
find src -name "*.java" -exec cat {} + | \
  docker exec -i ollama ollama run code-reviewer "ƒê√°nh gi√° ki·∫øn tr√∫c project"
```

---

### 4. T√≠ch H·ª£p IDE

#### VS Code - Continue.dev

1. **C√†i extension**: Search "Continue" trong VS Code Extensions
2. **Config**: Ch·ªânh `~/.continue/config.json`

```json
{
  "models": [
    {
      "title": "Qwen3-Coder 30B",
      "provider": "ollama",
      "model": "qwen3-coder:30b",
      "apiBase": "http://localhost:11434",
      "contextLength": 65536
    }
  ],
  "tabAutocompleteModel": {
    "title": "Qwen3-Coder Autocomplete",
    "provider": "ollama",
    "model": "qwen3-coder:30b",
    "contextLength": 4096
  }
}
```

**Ph√≠m t·∫Øt:**
- `Ctrl + L` - M·ªü chat panel
- `Ctrl + I` - Inline edit t·∫°i ch·ªó
- `Ctrl + Shift + L` - G·ª≠i code ƒëang ch·ªçn v√†o chat
- `Tab` - Ch·∫•p nh·∫≠n autocomplete

#### IntelliJ IDEA - CodeGPT

1. `Settings` ‚Üí `Plugins` ‚Üí Search "CodeGPT" ‚Üí Install
2. `Settings` ‚Üí `Tools` ‚Üí `CodeGPT`:
   - Provider: `Ollama`
   - Model: `qwen3-coder:30b`
   - URL: `http://localhost:11434`
3. Highlight code ‚Üí Right click ‚Üí **CodeGPT** ‚Üí Explain / Review / Refactor / Test

---

### 5. API Integration

#### cURL

```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder:30b",
    "messages": [
      {"role": "system", "content": "B·∫°n l√† Senior Java Developer."},
      {"role": "user", "content": "Vi·∫øt custom exception handler cho Spring Boot"}
    ],
    "temperature": 0.3
  }'
```

#### Python

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="not-needed"
)

response = client.chat.completions.create(
    model="qwen3-coder:30b",
    messages=[
        {"role": "system", "content": "B·∫°n l√† Java expert."},
        {"role": "user", "content": "Vi·∫øt DTO validation v·ªõi Jakarta Validation"}
    ],
    temperature=0.3
)

print(response.choices[0].message.content)
```

#### Java (OkHttp)

```java
import okhttp3.*;
import com.google.gson.*;

OkHttpClient client = new OkHttpClient.Builder()
    .readTimeout(120, TimeUnit.SECONDS)
    .build();

JsonObject body = new JsonObject();
body.addProperty("model", "qwen3-coder:30b");

JsonArray messages = new JsonArray();
JsonObject sysMsg = new JsonObject();
sysMsg.addProperty("role", "system");
sysMsg.addProperty("content", "B·∫°n l√† Java expert.");
messages.add(sysMsg);

JsonObject userMsg = new JsonObject();
userMsg.addProperty("role", "user");
userMsg.addProperty("content", "Vi·∫øt Spring Boot REST controller");
messages.add(userMsg);

body.add("messages", messages);
body.addProperty("temperature", 0.3);

Request request = new Request.Builder()
    .url("http://localhost:11434/v1/chat/completions")
    .post(RequestBody.create(body.toString(),
          MediaType.parse("application/json")))
    .build();

Response response = client.newCall(request).execute();
System.out.println(response.body().string());
```

---

## üîß Qu·∫£n L√Ω H·ªá Th·ªëng

### Docker Commands

```bash
# Kh·ªüi ƒë·ªông
docker compose up -d

# D·ª´ng (gi·ªØ data)
docker compose down

# D·ª´ng + x√≥a data (reset ho√†n to√†n)
docker compose down -v

# Restart
docker compose restart

# Xem logs
docker compose logs -f              # T·∫•t c·∫£
docker compose logs -f ollama       # Ch·ªâ Ollama
docker compose logs -f open-webui   # Ch·ªâ WebUI

# Health check
./scripts/health-check.sh
```

### Ollama Commands

```bash
# Li·ªát k√™ models
docker exec ollama ollama list

# Xem model ƒëang ch·∫°y
docker exec ollama ollama ps

# D·ª´ng model (gi·∫£i ph√≥ng RAM)
docker exec ollama ollama stop qwen3-coder:30b

# X√≥a model
docker exec ollama ollama rm <model-name>

# T·∫£i th√™m model
docker exec ollama ollama pull <model-name>
```

### Backup v√† Restore

```bash
# Backup model data (Docker volume)
docker run --rm -v ollama-data:/data -v $(pwd)/backup:/backup \
  alpine tar czf /backup/ollama-backup.tar.gz -C /data .

# Restore
docker run --rm -v ollama-data:/data -v $(pwd)/backup:/backup \
  alpine tar xzf /backup/ollama-backup.tar.gz -C /data
```

---

## ‚ö° Hi·ªáu NƒÉng

### RAM Usage (32GB Total)

| Th√†nh ph·∫ßn | RAM |
|------------|-----|
| OS | ~4 GB |
| Ollama + Model (Q4) | ~18-20 GB |
| Context window (32K) | ~2 GB |
| Open WebUI | ~0.5 GB |
| **T·ªïng** | **~25 GB** |
| **C√≤n l·∫°i** | **~7 GB** |

### T·ªëc ƒê·ªô D·ª± Ki·∫øn

- **CPU Only (i5-14400)**: 12-15 tokens/s
- **Context 32K**: ƒê·ªçc ƒë∆∞·ª£c ~800-1,000 d√≤ng code
- **Context 65K** (Code Reviewer): ƒê·ªçc ƒë∆∞·ª£c ~1,600-2,000 d√≤ng code

### ƒêi·ªÅu Ch·ªânh Context

Trong Modelfile ho·∫∑c khi g·ªçi API:

```bash
# 8K - chat ng·∫Øn, generate code nh·ªè
PARAMETER num_ctx 8192

# 32K - review 1-2 file (m·∫∑c ƒë·ªãnh)
PARAMETER num_ctx 32768

# 65K - ph√¢n t√≠ch c·∫£ package
PARAMETER num_ctx 65536
```

---

## üêõ X·ª≠ L√Ω L·ªói

### Container kh√¥ng kh·ªüi ƒë·ªông

```bash
docker compose logs ollama
docker compose logs open-webui
```

### Ollama OOM (Out of Memory)

```bash
# Gi·∫£m context trong Modelfile
PARAMETER num_ctx 8192

# Ho·∫∑c d√πng b·∫£n quantize nh·∫π h∆°n
docker exec ollama ollama pull qwen3-coder:30b-a3b-q3_K_M
```

### T·ªëc ƒë·ªô ch·∫≠m (< 5 tok/s)

```bash
# Ki·ªÉm tra CPU h·ªó tr·ª£ AVX2
grep -o 'avx2' /proc/cpuinfo | head -1

# Ki·ªÉm tra RAM kh·∫£ d·ª•ng
free -h

# N·∫øu swap ƒëang d√πng nhi·ªÅu ‚Üí RAM kh√¥ng ƒë·ªß
# ‚Üí Gi·∫£m context ho·∫∑c d√πng model nh·ªè h∆°n
```

### Model tr·∫£ l·ªùi k√©m ch·∫•t l∆∞·ª£ng

```bash
# Gi·∫£m temperature cho code generation
# Trong Modelfile:
PARAMETER temperature 0.2

# Vi·∫øt prompt c·ª• th·ªÉ h∆°n
# Thay v√¨: "vi·∫øt code"
# D√πng: "Vi·∫øt Java 17 Spring Boot 3 REST controller cho Product entity
#        v·ªõi CRUD, JPA, Jakarta Validation, custom exception handling"
```

---

## üìä Custom Model Personas

### Personas c√≥ s·∫µn

| Persona | Model | M√¥ t·∫£ | L·ªánh ch·∫°y |
|---------|-------|--------|-----------|
| **Qwen3-Coder** | Base | Model g·ªëc, ƒëa nƒÉng | `ollama run qwen3-coder:30b` |
| **Java Expert** | Custom | Senior Java Dev, Spring Boot, SOLID | `ollama run java-expert` |
| **Code Reviewer** | Custom | Review theo framework, severity levels | `ollama run code-reviewer` |

### T·∫°o Persona M·ªõi

1. T·∫°o file trong `modelfiles/`:

```
FROM qwen3-coder:30b

PARAMETER temperature 0.3
PARAMETER num_ctx 32768

SYSTEM """
M√¥ t·∫£ vai tr√≤ v√† quy t·∫Øc ·ªü ƒë√¢y...
"""
```

2. Build:

```bash
docker exec ollama ollama create <t√™n> -f /modelfiles/<t√™n>.modelfile
```

---

## üîí B·∫£o M·∫≠t

- ‚úÖ **100% Offline**: Sau khi c√†i ƒë·∫∑t, ng·∫Øt m·∫°ng v·∫´n ho·∫°t ƒë·ªông
- ‚úÖ **No Data Leak**: D·ªØ li·ªáu kh√¥ng r·ªùi kh·ªèi m√°y
- ‚úÖ **Local Storage**: Models v√† data l∆∞u trong Docker volumes
- ‚úÖ **SSH Tunnel**: Chia s·∫ª an to√†n cho m√°y kh√°c qua m√£ h√≥a

---

## üìö Resources

| Resource | Link |
|----------|------|
| README ƒë·∫ßy ƒë·ªß | [README.md](README.md) |
| Ollama Docs | [github.com/ollama/ollama](https://github.com/ollama/ollama) |
| Qwen3-Coder | [github.com/QwenLM/Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder) |
| Open WebUI | [github.com/open-webui/open-webui](https://github.com/open-webui/open-webui) |
| Continue.dev | [continue.dev](https://continue.dev) |
| CodeGPT | [JetBrains Marketplace](https://plugins.jetbrains.com/plugin/21056-codegpt) |

---

## ‚öôÔ∏è Th√¥ng S·ªë K·ªπ Thu·∫≠t

### Model: Qwen3-Coder 30B (A3B)

- **Total Params**: 30.5B
- **Active Params**: 3.3B (Mixture of Experts)
- **Architecture**: Transformer-based MoE
- **Quantization**: Q4_K_M (4-bit)
- **Model Size on Disk**: ~18GB
- **RAM Required**: ~20GB (loaded)
- **Context Window**: 256K tokens max
- **Training Data**: 7.5T tokens (70% code)
- **Languages**: 100+ programming languages
- **License**: Apache 2.0

### System Requirements Met

- ‚úÖ **OS**: Windows 11 Home Single Language
- ‚úÖ **CPU**: Intel Core i5-14400 (10 cores, 16 threads, AVX2)
- ‚úÖ **RAM**: 32GB (31.77GB available)
- ‚úÖ **Disk**: 56GB free space
- ‚úÖ **Docker**: 28.4.0

---

> üí° **M·∫πo**: Model ch·∫°y nhanh h∆°n sau l·∫ßn ƒë·∫ßu load v√†o RAM. Gi·ªØ model trong RAM b·∫±ng c√°ch set `OLLAMA_KEEP_ALIVE=30m` trong `.env`

> ‚ö†Ô∏è **L∆∞u √Ω**: M·ªçi d·ªØ li·ªáu ho√†n to√†n local. Kh√¥ng c√≥ g√¨ ƒë∆∞·ª£c g·ª≠i ra internet sau khi c√†i ƒë·∫∑t.

---

**Ng√†y t·∫°o**: 2026-02-24
**Version**: 1.0.0
