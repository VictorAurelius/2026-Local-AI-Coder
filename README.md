# Local AI Coder

> Há»‡ thá»‘ng AI há»— trá»£ láº­p trÃ¬nh cháº¡y hoÃ n toÃ n local, triá»ƒn khai báº±ng Docker.
> Dá»¯ liá»‡u khÃ´ng rá»i khá»i mÃ¡y. KhÃ´ng phá»¥ thuá»™c internet sau khi cÃ i Ä‘áº·t.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Local AI Coder                             â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Ollama   â”‚â—„â”€â”€â”‚  Open WebUI   â”‚    â”‚  IDE / Terminal   â”‚    â”‚
â”‚   â”‚  :11434   â”‚    â”‚  :3000        â”‚    â”‚  Continue.dev     â”‚    â”‚
â”‚   â”‚           â”‚    â”‚  (Chat UI)    â”‚    â”‚  CodeGPT          â”‚    â”‚
â”‚   â”‚  Qwen3-   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚   â”‚  Coder    â”‚                                                 â”‚
â”‚   â”‚  30B      â”‚â—„â”€â”€ SSH Tunnel (mÃ£ hÃ³a) â—„â”€â”€ MÃ¡y Client          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Má»¥c lá»¥c

1. [Tá»•ng quan](#1-tá»•ng-quan)
2. [YÃªu cáº§u há»‡ thá»‘ng](#2-yÃªu-cáº§u-há»‡-thá»‘ng)
3. [Cáº¥u trÃºc project](#3-cáº¥u-trÃºc-project)
4. [CÃ i Ä‘áº·t nhanh](#4-cÃ i-Ä‘áº·t-nhanh)
5. [CÃ i Ä‘áº·t chi tiáº¿t](#5-cÃ i-Ä‘áº·t-chi-tiáº¿t)
6. [Sá»­ dá»¥ng](#6-sá»­-dá»¥ng)
7. [TÃ­ch há»£p IDE](#7-tÃ­ch-há»£p-ide)
8. [Chia sáº» cho mÃ¡y khÃ¡c qua SSH Tunnel](#8-chia-sáº»-cho-mÃ¡y-khÃ¡c-qua-ssh-tunnel)
9. [Custom Model Personas](#9-custom-model-personas)
10. [Quáº£n lÃ½ vÃ  váº­n hÃ nh](#10-quáº£n-lÃ½-vÃ -váº­n-hÃ nh)
11. [Tá»‘i Æ°u hiá»‡u nÄƒng](#11-tá»‘i-Æ°u-hiá»‡u-nÄƒng)
12. [Xá»­ lÃ½ lá»—i](#12-xá»­-lÃ½-lá»—i)
13. [Tham kháº£o](#13-tham-kháº£o)

---

## 1. Tá»•ng quan

### Váº¥n Ä‘á»

KhÃ¡ch hÃ ng yÃªu cáº§u AI há»— trá»£ phÃ¢n tÃ­ch vÃ  sinh code Java nhÆ°ng **khÃ´ng cháº¥p nháº­n gá»­i dá»¯ liá»‡u lÃªn cloud** (GitHub Copilot, ChatGPT, Claude Code...) vÃ¬ lo ngáº¡i báº£o máº­t vÃ  lá»™ mÃ£ nguá»“n.

### Giáº£i phÃ¡p

Triá»ƒn khai **Qwen3-Coder 30B (A3B)** â€” model AI chuyÃªn code máº¡nh nháº¥t cÃ³ thá»ƒ cháº¡y trÃªn pháº§n cá»©ng phá»• thÃ´ng â€” hoÃ n toÃ n local báº±ng Docker. KÃ¨m giao diá»‡n web, tÃ­ch há»£p IDE, vÃ  kháº£ nÄƒng chia sáº» an toÃ n cho mÃ¡y khÃ¡c qua SSH Tunnel.

### Model: Qwen3-Coder 30B (A3B)

| ThÃ´ng sá»‘              | GiÃ¡ trá»‹                                      |
| --------------------- | --------------------------------------------- |
| Parameters tá»•ng       | 30.5B                                         |
| Parameters kÃ­ch hoáº¡t  | 3.3B (Mixture of Experts â€” nhanh nhÆ° model 3B)|
| Context window        | **256K tokens** (Ä‘á»c Ä‘Æ°á»£c ~6,000 dÃ²ng code)   |
| Training data         | 7.5 nghÃ¬n tá»· tokens (70% code)                |
| NgÃ´n ngá»¯ láº­p trÃ¬nh    | 100+ (Java, Python, JS, Go, Rust...)          |
| License               | Apache 2.0 (thÆ°Æ¡ng máº¡i tá»± do)                  |

### So sÃ¡nh vá»›i Qwen 2.5 Coder 7B (model cÅ©)

| TiÃªu chÃ­              | Qwen 2.5 Coder 7B | Qwen3-Coder 30B     |
| ---------------------- | ------------------ | -------------------- |
| Context window         | 32K tokens         | **256K tokens (8x)** |
| Äá»c file code dÃ i      | Yáº¿u                | **Ráº¥t tá»‘t**         |
| Tá»‘c Ä‘á»™ CPU (32GB RAM) | 4â€“7 tok/s          | **12â€“15 tok/s**      |
| Code quality           | Tá»‘t                | **VÆ°á»£t trá»™i**        |
| Agentic / Tool calling | KhÃ´ng              | **CÃ³**               |

---

## 2. YÃªu cáº§u há»‡ thá»‘ng

### MÃ¡y Server (cháº¡y model)

| ThÃ nh pháº§n | Tá»‘i thiá»ƒu              | Khuyáº¿n nghá»‹           |
| ---------- | ---------------------- | --------------------- |
| OS         | Linux / macOS / Win 10+| Ubuntu 22.04+ LTS     |
| CPU        | AVX2 support           | Intel i7+ / AMD Ryzen 7+ |
| RAM        | 32 GB                  | 32 GB                 |
| á»” Ä‘Ä©a     | 30 GB trá»‘ng            | SSD NVMe 50 GB+       |
| Docker     | Docker 24+             | Docker 27+             |
| Máº¡ng       | Cáº§n khi táº£i model      | KhÃ´ng cáº§n sau khi cÃ i  |

### MÃ¡y Client (náº¿u dÃ¹ng SSH Tunnel)

| ThÃ nh pháº§n | YÃªu cáº§u                              |
| ---------- | ------------------------------------- |
| OS         | Báº¥t ká»³ (Linux / macOS / Windows)      |
| SSH Client | CÃ³ sáºµn trÃªn háº§u háº¿t OS               |
| Ollama CLI | TÃ¹y chá»n (cÃ³ thá»ƒ dÃ¹ng API thay tháº¿)  |

---

## 3. Cáº¥u trÃºc project

```
local-ai-coder/
â”œâ”€â”€ docker-compose.yml              # Docker services: Ollama + Open WebUI
â”œâ”€â”€ .env.example                    # Cáº¥u hÃ¬nh máº«u (copy â†’ .env)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                       # TÃ i liá»‡u nÃ y
â”‚
â”œâ”€â”€ modelfiles/                     # Custom model personas
â”‚   â”œâ”€â”€ JavaExpert.modelfile        # Senior Java Developer persona
â”‚   â””â”€â”€ CodeReviewer.modelfile      # Code Reviewer persona
â”‚
â”œâ”€â”€ nginx/                          # Reverse proxy (tÃ¹y chá»n)
â”‚   â””â”€â”€ ollama-proxy.conf           # Nginx config + IP whitelist
â”‚
â””â”€â”€ scripts/                        # Scripts tiá»‡n Ã­ch
    â”œâ”€â”€ setup.sh                    # CÃ i Ä‘áº·t tá»± Ä‘á»™ng (1 lá»‡nh)
    â”œâ”€â”€ health-check.sh             # Kiá»ƒm tra tráº¡ng thÃ¡i há»‡ thá»‘ng
    â”œâ”€â”€ review.sh                   # Gá»­i file code Ä‘á»ƒ review
    â”œâ”€â”€ tunnel-server-setup.sh      # Setup SSH trÃªn server
    â””â”€â”€ tunnel-client-connect.sh    # Káº¿t ná»‘i tunnel tá»« client
```

---

## 4. CÃ i Ä‘áº·t nhanh

DÃ nh cho ngÆ°á»i muá»‘n cháº¡y nhanh nháº¥t cÃ³ thá»ƒ:

```bash
# 1. Clone project
git clone <repo-url> local-ai-coder
cd local-ai-coder

# 2. Táº¡o file cáº¥u hÃ¬nh
cp .env.example .env

# 3. Cháº¡y setup tá»± Ä‘á»™ng (cÃ i táº¥t cáº£, táº£i model)
chmod +x scripts/*.sh
./scripts/setup.sh

# 4. Sá»­ dá»¥ng
docker exec -it ollama ollama run qwen3-coder:30b    # Chat terminal
# hoáº·c má»Ÿ http://localhost:3000                       # Web UI
```

> â±ï¸ Láº§n Ä‘áº§u cháº¡y cáº§n táº£i model ~18 GB. CÃ¡c láº§n sau khá»Ÿi Ä‘á»™ng trong vÃ i giÃ¢y.

---

## 5. CÃ i Ä‘áº·t chi tiáº¿t

### 5.1. CÃ i Docker

**Linux (Ubuntu/Debian):**

```bash
# CÃ i Docker Engine
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER
newgrp docker

# Kiá»ƒm tra
docker --version
docker compose version
```

**macOS:**

Táº£i Docker Desktop tá»« [https://docker.com/get-docker](https://docker.com/get-docker).

**Windows:**

Táº£i Docker Desktop tá»« [https://docker.com/get-docker](https://docker.com/get-docker). Báº­t WSL 2 khi Ä‘Æ°á»£c há»i.

### 5.2. Clone vÃ  cáº¥u hÃ¬nh

```bash
git clone <repo-url> local-ai-coder
cd local-ai-coder

# Táº¡o file cáº¥u hÃ¬nh
cp .env.example .env
```

Chá»‰nh `.env` náº¿u cáº§n:

```bash
# .env â€” CÃ¡c giÃ¡ trá»‹ máº·c Ä‘á»‹nh thÆ°á»ng phÃ¹ há»£p cho háº§u háº¿t trÆ°á»ng há»£p

MODEL_NAME=qwen3-coder:30b     # Model chÃ­nh
OLLAMA_PORT=11434               # Port API
WEBUI_PORT=3000                 # Port giao diá»‡n web
OLLAMA_MEMORY_LIMIT=28g        # Giá»›i háº¡n RAM cho Docker
OLLAMA_KEEP_ALIVE=5m           # Giá»¯ model trong RAM bao lÃ¢u sau láº§n dÃ¹ng cuá»‘i
```

### 5.3. Khá»Ÿi Ä‘á»™ng containers

```bash
docker compose up -d
```

Docker sáº½ táº£i images (láº§n Ä‘áº§u ~2â€“3 GB) vÃ  khá»Ÿi Ä‘á»™ng 2 services:

| Service    | MÃ´ táº£             | Port  |
| ---------- | ------------------ | ----- |
| `ollama`   | Engine cháº¡y model  | 11434 |
| `open-webui` | Giao diá»‡n chat web | 3000  |

Kiá»ƒm tra:

```bash
docker compose ps

# Output:
# NAME        STATUS              PORTS
# ollama      Up (healthy)        0.0.0.0:11434->11434/tcp
# open-webui  Up                  0.0.0.0:3000->8080/tcp
```

### 5.4. Táº£i model

```bash
# Táº£i Qwen3-Coder 30B (~18 GB, cáº§n máº¡ng tá»‘t)
docker exec ollama ollama pull qwen3-coder:30b
```

> ğŸ’¡ **Máº¹o:** Náº¿u máº¡ng cháº­m hoáº·c hay ngáº¯t, Ollama há»— trá»£ táº£i tiáº¿p (resume). Cháº¡y láº¡i lá»‡nh trÃªn náº¿u bá»‹ giÃ¡n Ä‘oáº¡n.

### 5.5. Táº¡o custom model personas

```bash
# Java Expert â€” chuyÃªn Java/Spring Boot
docker exec ollama ollama create java-expert -f /modelfiles/JavaExpert.modelfile

# Code Reviewer â€” review code theo framework
docker exec ollama ollama create code-reviewer -f /modelfiles/CodeReviewer.modelfile
```

### 5.6. XÃ¡c nháº­n cÃ i Ä‘áº·t

```bash
./scripts/health-check.sh
```

Hoáº·c kiá»ƒm tra thá»§ cÃ´ng:

```bash
# API
curl http://localhost:11434/api/version

# Liá»‡t kÃª models
docker exec ollama ollama list

# Test generate
docker exec -it ollama ollama run qwen3-coder:30b "Viáº¿t hello world Java"
```

### 5.7. Sau khi cÃ i xong â€” ngáº¯t máº¡ng

Sau khi model Ä‘Ã£ táº£i, cÃ³ thá»ƒ **ngáº¯t hoÃ n toÃ n káº¿t ná»‘i internet**. Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng offline 100%.

---

## 6. Sá»­ dá»¥ng

### 6.1. Chat qua Terminal

```bash
# DÃ¹ng model gá»‘c
docker exec -it ollama ollama run qwen3-coder:30b

# DÃ¹ng Java Expert persona
docker exec -it ollama ollama run java-expert

# DÃ¹ng Code Reviewer persona
docker exec -it ollama ollama run code-reviewer
```

Trong chat session, gÃµ yÃªu cáº§u:

```
>>> Viáº¿t Spring Boot 3 REST controller CRUD cho entity Product,
    sá»­ dá»¥ng JPA, Jakarta Validation, ResponseEntity, vÃ  custom exception handling
```

ThoÃ¡t: `/bye` hoáº·c `Ctrl + D`.

### 6.2. Chat qua Web UI

1. Má»Ÿ trÃ¬nh duyá»‡t â†’ `http://localhost:3000`.
2. Táº¡o tÃ i khoáº£n admin (láº§n Ä‘áº§u, chá»‰ lÆ°u local).
3. Chá»n model tá»« dropdown â†’ chat.
4. KÃ©o tháº£ file Ä‘á»ƒ upload phÃ¢n tÃ­ch.

### 6.3. PhÃ¢n tÃ­ch file code

```bash
# Review 1 file
./scripts/review.sh src/main/java/com/example/UserService.java

# Review cáº£ folder
./scripts/review.sh src/main/java/com/example/service/

# Review vá»›i prompt tÃ¹y chá»‰nh
./scripts/review.sh OrderController.java "Táº­p trung vÃ o security vÃ  SQL injection"
```

Hoáº·c pipe trá»±c tiáº¿p:

```bash
# 1 file
cat UserService.java | docker exec -i ollama ollama run code-reviewer "Review code nÃ y"

# Nhiá»u file
cat src/main/java/com/example/service/*.java | \
  docker exec -i ollama ollama run code-reviewer "PhÃ¢n tÃ­ch toÃ n bá»™ service layer"

# Cáº£ project
find src -name "*.java" -exec cat {} + | \
  docker exec -i ollama ollama run code-reviewer "ÄÃ¡nh giÃ¡ kiáº¿n trÃºc project"
```

### 6.4. Gá»i API (tÆ°Æ¡ng thÃ­ch OpenAI format)

```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder:30b",
    "messages": [
      {"role": "system", "content": "Báº¡n lÃ  Senior Java Developer."},
      {"role": "user", "content": "Viáº¿t custom exception handler cho Spring Boot"}
    ]
  }'
```

### 6.5. TÃ­ch há»£p Python

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="not-needed"
)

response = client.chat.completions.create(
    model="qwen3-coder:30b",
    messages=[
        {"role": "system", "content": "Báº¡n lÃ  Java expert."},
        {"role": "user", "content": "Viáº¿t DTO validation vá»›i Jakarta Validation"}
    ],
    temperature=0.3
)
print(response.choices[0].message.content)
```

### 6.6. TÃ­ch há»£p Java (OkHttp)

```java
OkHttpClient client = new OkHttpClient.Builder()
        .readTimeout(120, TimeUnit.SECONDS)
        .build();

JsonObject body = new JsonObject();
body.addProperty("model", "qwen3-coder:30b");
// ... (xem chi tiáº¿t trong pháº§n Phá»¥ lá»¥c)

Request request = new Request.Builder()
        .url("http://localhost:11434/v1/chat/completions")
        .post(RequestBody.create(body.toString(), MediaType.parse("application/json")))
        .build();
```

---

## 7. TÃ­ch há»£p IDE

### 7.1. VS Code â€” Continue.dev

1. Extensions â†’ tÃ¬m **"Continue"** â†’ Install.
2. Chá»‰nh `~/.continue/config.json`:

```json
{
  "models": [
    {
      "title": "Qwen3-Coder 30B",
      "provider": "ollama",
      "model": "qwen3-coder:30b",
      "apiBase": "http://localhost:11434",
      "contextLength": 65536
    }
  ],
  "tabAutocompleteModel": {
    "title": "Qwen3-Coder Autocomplete",
    "provider": "ollama",
    "model": "qwen3-coder:30b",
    "contextLength": 4096
  }
}
```

PhÃ­m táº¯t:

| PhÃ­m               | Chá»©c nÄƒng                  |
| ------------------- | -------------------------- |
| `Ctrl + L`          | Má»Ÿ chat panel              |
| `Ctrl + I`          | Inline edit táº¡i chá»—         |
| `Ctrl + Shift + L`  | Gá»­i code Ä‘ang chá»n vÃ o chat |
| `Tab`               | Cháº¥p nháº­n autocomplete      |

### 7.2. IntelliJ IDEA â€” CodeGPT

1. `Settings` â†’ `Plugins` â†’ tÃ¬m **"CodeGPT"** â†’ Install.
2. `Settings` â†’ `Tools` â†’ `CodeGPT`:
   - Provider: `Ollama`
   - Model: `qwen3-coder:30b`
   - URL: `http://localhost:11434`
3. Highlight code â†’ chuá»™t pháº£i â†’ **CodeGPT** â†’ Explain / Review / Refactor / Test.

---

## 8. Chia sáº» cho mÃ¡y khÃ¡c qua SSH Tunnel

Khi model cháº¡y trÃªn 1 mÃ¡y server vÃ  báº¡n muá»‘n **duy nháº¥t 1 mÃ¡y client** truy cáº­p:

```
MÃ¡y SERVER                   SSH Tunnel (mÃ£ hÃ³a)              MÃ¡y CLIENT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker       â”‚       â”‚                      â”‚       â”‚ IDE          â”‚
â”‚  Ollama      â”‚â—„â”€â”€â”€â”€â”€â”€â”‚  Port 11434 â—„â”€â”€â”€â”€â”€â”€â–º â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ Terminal     â”‚
â”‚  :11434      â”‚       â”‚  (chá»‰ ai cÃ³ SSH key  â”‚       â”‚ localhost    â”‚
â”‚  (localhost) â”‚       â”‚   má»›i dÃ¹ng Ä‘Æ°á»£c)     â”‚       â”‚  :11434      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.1. Setup Server

```bash
# TrÃªn mÃ¡y server â€” cháº¡y script setup
./scripts/tunnel-server-setup.sh
```

Script sáº½ kiá»ƒm tra SSH server, táº¡o user riÃªng cho tunnel (tÃ¹y chá»n), vÃ  hiá»ƒn thá»‹ hÆ°á»›ng dáº«n.

Hoáº·c setup thá»§ cÃ´ng:

```bash
# Äáº£m báº£o SSH server Ä‘ang cháº¡y
sudo systemctl enable ssh
sudo systemctl start ssh

# Ollama giá»¯ nguyÃªn localhost:11434 â€” KHÃ”NG expose ra ngoÃ i
```

### 8.2. Setup Client

```bash
# TrÃªn mÃ¡y client â€” copy .env.example vÃ  chá»‰nh thÃ´ng tin server
cp .env.example .env
# Sá»­a: SSH_SERVER_USER, SSH_SERVER_IP, SSH_SERVER_PORT, SSH_KEY_PATH

# Cháº¡y script káº¿t ná»‘i
./scripts/tunnel-client-connect.sh
```

Hoáº·c káº¿t ná»‘i thá»§ cÃ´ng:

```bash
# Táº¡o SSH key (láº§n Ä‘áº§u)
ssh-keygen -t ed25519 -C "ollama-tunnel"
ssh-copy-id user@192.168.1.100

# Táº¡o tunnel
ssh -f -N -L 11434:localhost:11434 \
    -o ServerAliveInterval=30 \
    -o ServerAliveCountMax=3 \
    user@192.168.1.100

# DÃ¹ng model (y há»‡t nhÆ° model Ä‘ang cháº¡y local)
ollama run qwen3-coder:30b
curl http://localhost:11434/v1/chat/completions ...
```

### 8.3. Tá»± Ä‘á»™ng káº¿t ná»‘i khi khá»Ÿi Ä‘á»™ng (Linux client)

```bash
sudo nano /etc/systemd/system/ollama-tunnel.service
```

```ini
[Unit]
Description=SSH Tunnel to Ollama Server
After=network-online.target

[Service]
Type=simple
User=your-username
ExecStart=/usr/bin/ssh -N -L 11434:localhost:11434 \
    -o ServerAliveInterval=30 \
    -o ServerAliveCountMax=3 \
    -o ExitOnForwardFailure=yes \
    -i /home/your-username/.ssh/id_ed25519 \
    user@192.168.1.100
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl enable ollama-tunnel
sudo systemctl start ollama-tunnel
```

### 8.4. Báº£o máº­t nÃ¢ng cao

Giá»›i háº¡n SSH key chá»‰ Ä‘Æ°á»£c dÃ¹ng cho tunnel (trÃªn server):

```bash
# Trong /home/ollama-tunnel/.ssh/authorized_keys, thÃªm prefix:
no-agent-forwarding,no-X11-forwarding,permitopen="localhost:11434",command="/bin/false" ssh-ed25519 AAAA... client-key
```

Káº¿t quáº£: key nÃ y chá»‰ táº¡o Ä‘Æ°á»£c tunnel tá»›i port 11434, khÃ´ng thá»ƒ cháº¡y lá»‡nh hay truy cáº­p gÃ¬ khÃ¡c trÃªn server.

---

## 9. Custom Model Personas

Project bao gá»“m 2 persona máº«u trong `modelfiles/`. Báº¡n cÃ³ thá»ƒ táº¡o thÃªm.

### Personas cÃ³ sáºµn

| Persona | File | MÃ´ táº£ | Lá»‡nh cháº¡y |
|---------|------|--------|------------|
| **Java Expert** | `JavaExpert.modelfile` | Senior Java Dev, Spring Boot, SOLID | `ollama run java-expert` |
| **Code Reviewer** | `CodeReviewer.modelfile` | Review theo framework cÃ³ severity levels | `ollama run code-reviewer` |

### Táº¡o persona má»›i

1. Táº¡o file trong `modelfiles/`:

```
FROM qwen3-coder:30b

PARAMETER temperature 0.3
PARAMETER num_ctx 32768

SYSTEM """
MÃ´ táº£ vai trÃ² vÃ  quy táº¯c á»Ÿ Ä‘Ã¢y...
"""
```

2. Build:

```bash
docker exec ollama ollama create <tÃªn> -f /modelfiles/<tÃªn>.modelfile
```

### VÃ­ dá»¥ thÃªm personas

**API Designer:**

```
FROM qwen3-coder:30b
PARAMETER temperature 0.3
PARAMETER num_ctx 32768
SYSTEM """
Báº¡n lÃ  API Designer chuyÃªn thiáº¿t káº¿ RESTful API.
TuÃ¢n thá»§ OpenAPI 3.0, JSON:API conventions, proper HTTP status codes.
LuÃ´n generate kÃ¨m Swagger/OpenAPI spec.
"""
```

**Database Expert:**

```
FROM qwen3-coder:30b
PARAMETER temperature 0.2
PARAMETER num_ctx 32768
SYSTEM """
Báº¡n lÃ  DBA chuyÃªn PostgreSQL vÃ  query optimization.
PhÃ¢n tÃ­ch query plan, Ä‘á» xuáº¥t indexes, normalization.
"""
```

---

## 10. Quáº£n lÃ½ vÃ  váº­n hÃ nh

### Docker commands

```bash
# Khá»Ÿi Ä‘á»™ng
docker compose up -d

# Dá»«ng (giá»¯ data)
docker compose down

# Dá»«ng + xÃ³a data (reset hoÃ n toÃ n)
docker compose down -v

# Restart
docker compose restart

# Xem logs
docker compose logs -f              # Táº¥t cáº£
docker compose logs -f ollama       # Chá»‰ Ollama
docker compose logs -f open-webui   # Chá»‰ WebUI

# Health check
./scripts/health-check.sh
```

### Ollama commands (cháº¡y trong container)

```bash
# Liá»‡t kÃª models
docker exec ollama ollama list

# Xem model Ä‘ang cháº¡y
docker exec ollama ollama ps

# Dá»«ng model (giáº£i phÃ³ng RAM)
docker exec ollama ollama stop qwen3-coder:30b

# XÃ³a model
docker exec ollama ollama rm <model-name>

# Táº£i thÃªm model
docker exec ollama ollama pull <model-name>
```

### Backup vÃ  restore

```bash
# Backup model data (Docker volume)
docker run --rm -v ollama-data:/data -v $(pwd)/backup:/backup \
  alpine tar czf /backup/ollama-backup.tar.gz -C /data .

# Restore
docker run --rm -v ollama-data:/data -v $(pwd)/backup:/backup \
  alpine tar xzf /backup/ollama-backup.tar.gz -C /data
```

### Cho team truy cáº­p Web UI qua LAN

Open WebUI máº·c Ä‘á»‹nh bind `0.0.0.0:3000`. Báº¥t ká»³ mÃ¡y nÃ o trong LAN Ä‘á»u truy cáº­p Ä‘Æ°á»£c:

```
http://<server-ip>:3000
```

Náº¿u muá»‘n giá»›i háº¡n, bá» comment khá»‘i `nginx` trong `docker-compose.yml` vÃ  chá»‰nh IP whitelist trong `nginx/ollama-proxy.conf`.

---

## 11. Tá»‘i Æ°u hiá»‡u nÄƒng

### RAM budget cho 32GB

| ThÃ nh pháº§n               | RAM       |
| ------------------------ | --------- |
| OS                       | ~4 GB     |
| Ollama + Model (Q4)     | ~18â€“20 GB |
| Context window (32K)     | ~2 GB     |
| Open WebUI               | ~0.5 GB   |
| **Tá»•ng**                | **~25 GB** |
| **CÃ²n láº¡i cho IDE/apps** | **~7 GB** |

### Äiá»u chá»‰nh context length

Trong `.env`:

```bash
# Hoáº·c chá»‰nh qua Modelfile: PARAMETER num_ctx 32768
```

| Context   | RAM thÃªm | PhÃ¹ há»£p cho                |
| --------- | -------- | --------------------------- |
| 8192      | +0.5 GB  | Chat, generate code ngáº¯n    |
| 32768     | +2 GB    | Review 1â€“2 file (máº·c Ä‘á»‹nh)  |
| 65536     | +4 GB    | PhÃ¢n tÃ­ch cáº£ package         |

### Tiáº¿t kiá»‡m RAM

```bash
# Giáº£m thá»i gian giá»¯ model trong RAM
# .env:
OLLAMA_KEEP_ALIVE=2m   # Unload model sau 2 phÃºt idle (máº·c Ä‘á»‹nh 5m)

# Chá»‰ cho phÃ©p 1 model cháº¡y cÃ¹ng lÃºc
OLLAMA_MAX_LOADED_MODELS=1
```

### Tá»‘i Æ°u há»‡ thá»‘ng Linux

```bash
# Táº¡o swap file 16GB (Ä‘á» phÃ²ng OOM)
sudo fallocate -l 16G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab

# TÄƒng swappiness
sudo sysctl vm.swappiness=60
```

---

## 12. Xá»­ lÃ½ lá»—i

### Container khÃ´ng khá»Ÿi Ä‘á»™ng

```bash
docker compose logs ollama
docker compose logs open-webui
```

### Ollama OOM (Out of Memory)

```bash
# Giáº£m context
# Trong Modelfile: PARAMETER num_ctx 8192

# Hoáº·c dÃ¹ng báº£n quantize nháº¹ hÆ¡n
docker exec ollama ollama pull qwen3-coder:30b-a3b-q3_K_M
```

### Tá»‘c Ä‘á»™ cháº­m (< 5 tok/s)

```bash
# Kiá»ƒm tra CPU há»— trá»£ AVX2
grep -o 'avx2' /proc/cpuinfo | head -1

# Kiá»ƒm tra RAM kháº£ dá»¥ng
free -h

# Náº¿u swap Ä‘ang dÃ¹ng nhiá»u â†’ RAM khÃ´ng Ä‘á»§ â†’ giáº£m context hoáº·c dÃ¹ng model nhá» hÆ¡n
```

### SSH Tunnel khÃ´ng káº¿t ná»‘i

```bash
# Debug verbose
ssh -v -N -L 11434:localhost:11434 user@server-ip

# Kiá»ƒm tra port Ä‘Ã£ bá»‹ chiáº¿m chÆ°a
lsof -i :11434   # Linux/macOS
netstat -an | findstr 11434   # Windows
```

### Model tráº£ lá»i kÃ©m cháº¥t lÆ°á»£ng

```bash
# Giáº£m temperature cho code generation
# Trong Modelfile: PARAMETER temperature 0.2

# Viáº¿t prompt cá»¥ thá»ƒ hÆ¡n:
# Thay vÃ¬: "viáº¿t code"
# DÃ¹ng: "Viáº¿t Java 17 Spring Boot 3 REST controller cho Product entity
#        vá»›i CRUD, JPA, Jakarta Validation, custom exception handling"
```

---

## 13. Tham kháº£o

| Resource | Link |
|----------|------|
| Ollama Documentation | [github.com/ollama/ollama](https://github.com/ollama/ollama) |
| Qwen3-Coder Official | [github.com/QwenLM/Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder) |
| Qwen3-Coder Local Guide | [unsloth.ai/docs/models/qwen3-coder](https://unsloth.ai/docs/models/qwen3-coder-how-to-run-locally) |
| Open WebUI | [github.com/open-webui/open-webui](https://github.com/open-webui/open-webui) |
| Continue.dev | [continue.dev](https://continue.dev) |
| CodeGPT Plugin | [JetBrains Marketplace](https://plugins.jetbrains.com/plugin/21056-codegpt) |

---

> ğŸ“ *Táº¡o ngÃ y 24/02/2026*
>
> **Má»i dá»¯ liá»‡u hoÃ n toÃ n local. KhÃ´ng cÃ³ gÃ¬ Ä‘Æ°á»£c gá»­i ra internet sau khi cÃ i Ä‘áº·t.**
